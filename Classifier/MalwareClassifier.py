import psutil
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn import tree
from sklearn.feature_extraction import DictVectorizer
from sklearn import metrics
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import validation_curve
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt


vectorizer = DictVectorizer(sparse=False)

features_df = pd.read_csv('D:/pythonProject/MalwareAnalysis/StaticAnalysis/Final table/Features.csv')
features_df['Target'].replace('Benign', 0, inplace=True)
features_df['Target'].replace('Malware', 1, inplace=True)


def create_data_set(data_frame):
    y = list()
    for target_list in data_frame.loc[:, ['Target']].values:
        for value in target_list:
            y.append(value)
    data_frame.drop(['Target'], inplace=True, axis=1)
    set = data_frame.T.to_dict().values()
    vectorizer.fit(set)
    X = vectorizer.transform(set)
    return X,y


X,y = create_data_set(features_df)
train_X, test_X, train_Y, test_Y = train_test_split(X, y, test_size=0.3, random_state=42)

# --------------------------------------------------------------------------------------------------------
# PCA
x_pca = train_X
y_pca = train_Y

# Standardize the Data
x_pca = StandardScaler().fit_transform(x_pca)

pca = PCA(n_components=2)
principalComponents = pca.fit_transform(x_pca)
principal_train_df = pd.DataFrame(data=principalComponents, columns=['Principal component 1'
                                                                    ,'Principal component 2'])
targetDf = pd.DataFrame(data=y_pca, columns=["Target"])

result_train_df = pd.concat([principal_train_df, targetDf], axis=1)

result_train_df['Target'].replace(0, 'Benign', inplace=True)
result_train_df['Target'].replace(1, 'Malware', inplace=True)

# Visualize 2D Projection
fig = plt.figure(figsize=(8, 8))
ax = fig.add_subplot(1, 1, 1)
# ax = plt.axes(projection='3d')
ax.set_xlabel('Principal Component 2', fontsize=15)
ax.set_ylabel('Principal Component 1', fontsize=15)
ax.set_title('2 Component PCA', fontsize=20)

targets = ['Malware', 'Benign']
colors = ['r', 'g']
for target, color in zip(targets, colors):
    indicesToKeep = result_train_df['Target'] == target
    ax.scatter(result_train_df.loc[indicesToKeep, 'Principal component 1']
               , result_train_df.loc[indicesToKeep, 'Principal component 2']
               , c=color
               , s=2)
ax.legend(targets)
ax.grid()

plt.show()


# --------------------------------------------------------------------------------------------------------
# Function for drawing plot

class CurveData:
    def __init__(self,param_range,score,color,label):
        self.param_range = param_range
        self.score = score
        self.color = color
        self.label = label


def draw_plot(train_curve,val_curve, x_lim, y_lim, x_label, y_label, title):

    # Calculating mean and standard deviation for training set scores
    train_mean = np.mean(train_curve.score, axis=1)
    train_std = np.std(train_curve.score, axis=1)

    # Calculating mean and standard deviation for validation set scores
    val_mean = np.mean(val_curve.score, axis=1)
    val_std = np.std(val_curve.score, axis=1)

    plt.subplots(1, figsize=(7, 7))
    plt.plot(train_curve.param_range, train_mean, color=train_curve.color, label=train_curve.label)
    plt.plot(val_curve.param_range, val_mean, color=val_curve.color, label=val_curve.label)
    plt.fill_between(val_curve.param_range, val_mean - val_std, val_mean + val_std, color="gainsboro")
    plt.legend(loc='best')
    plt.xlim(x_lim)
    plt.ylim(y_lim)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.title(title, size=14)

    plt.show()

# --------------------------------------------------------------------------------------------------------
# Training
# Decision Tree Algorithm

classifier = tree.DecisionTreeClassifier(criterion='entropy',class_weight="balanced", random_state=42)
classifier.fit(train_X, train_Y)

# Visualize the Decision Tree
with open('decisionTree.dot', 'w') as output_file:
    tree.export_graphviz(
        classifier,
        feature_names=vectorizer.get_feature_names(),
        out_file=output_file,
        filled=True,
        rounded=True
    )


pred_Y=classifier.predict(test_X)
print('Decision Tree Classifier on hold-out:',metrics.accuracy_score(test_Y, pred_Y))
score = cross_val_score(classifier, X, y, cv=5).mean()
print('Decision Tree Classifier on five-fold cross-validation:', score)

# Plotting Validation Curve
# Creating range of values for parameter
depth = np.arange(1, classifier.tree_.max_depth)

train_score, val_score = validation_curve(classifier,X, y, param_name="max_depth", cv=5
                                          ,param_range=depth,n_jobs=psutil.cpu_count()
                                          ,scoring="accuracy")
train_score_data = CurveData(depth, train_score, color='blue', label='training score')
val_score_data = CurveData(depth, val_score, color='red', label='validation score')
draw_plot(train_score_data,val_score_data,None,(0,1),'Maximum depth of the tree','Score','Validation Curve With Decision Tree')

# Plotting Learning Curve
N, train_lc, val_lc = learning_curve(classifier,X, y, cv=5,train_sizes=np.linspace(0.1, 1, 50))

train_lc_data = CurveData(N, train_lc, color='blue', label='training score')
val_lc_data = CurveData(N, val_lc, color='red', label='validation score')
draw_plot(train_lc_data,val_lc_data,(N[0], N[-1]),None,'Training size','Score','Learning Curve With Decision Tree')


# --------------------------------------------------------------------------------------------------------
# Training
# Random Forest Classifier

classifier = RandomForestClassifier(n_estimators=100, class_weight="balanced", random_state=42)
classifier.fit(train_X, train_Y)

pred_Y=classifier.predict(test_X)
print('Random Forest Classifier on hold-out: ',metrics.accuracy_score(test_Y, pred_Y))
score = cross_val_score(classifier, X, y, cv=5).mean()
print('Random Forest Classifier on five-fold cross-validation:',score)

# Plotting Validation Curve
# Creating range of values for parameter
trees_num = np.arange(1, 250, 2)

train_score, val_score = validation_curve(RandomForestClassifier(class_weight="balanced", random_state=42),
                                X, y, param_name="n_estimators", param_range=trees_num,
                                cv=5, scoring="accuracy", n_jobs=psutil.cpu_count())

train_score_data = CurveData(trees_num, train_score, color='navy', label='training score')
val_score_data = CurveData(trees_num, val_score, color='olive', label='validation score')
draw_plot(train_score_data,val_score_data,None,None,'Number Of Trees','Score','Validation Curve With Random Forest')

# Plotting Learning Curve
N, train_lc, val_lc = learning_curve(classifier,X, y, cv=5,train_sizes=np.linspace(0.1, 1, 50))

train_lc_data = CurveData(N, train_lc, color='navy', label='training score')
val_lc_data = CurveData(N, val_lc, color='olive', label='validation score')
draw_plot(train_lc_data,val_lc_data,(N[0], N[-1]),None,'Training size','Score','Learning Curve With Random Forest')


